{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "# !conda install --yes --prefix {sys.prefix} dill\n",
    "# !conda install --yes --prefix {sys.prefix} spacy\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "# !conda install --yes --prefix {sys.prefix} spacy\n",
    "# !{sys.executable} -m spacy download en\n",
    "# !{sys.executable} -m pip install contractions\n",
    "# !conda install --yes --prefix {sys.prefix} -c glemaitre imbalanced-learn\n",
    "# !conda install --yes --prefix {sys.prefix} gensim\n",
    "\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These might need to stay global until I can figure out how to include in the sklearn Pipeline.\n",
    "# nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "nlp = spacy.load('en')\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = stopwords.words('english')\n",
    "#we want the negatives\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def replace_accented_chars(text):\n",
    "    #The normal form KD (NFKD) will apply the compatibility decomposition, \n",
    "    #i.e. replace all compatibility characters with their equivalents (from python.org). \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=contractions.contractions_dict):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        if contraction_mapping.get(match):\n",
    "            expanded_contraction = contraction_mapping.get(match)\n",
    "        else:\n",
    "            expanded_contraction = contraction_mapping.get(match.lower())                    \n",
    "        if expanded_contraction:\n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "    \n",
    "def normalize_corpus(doc, html_stripping=True, contraction_expansion=True, text_lemmatization=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    def get_profanity():\n",
    "        file_path = os.path.join(os.getcwd(),\"corpora\",\"profanity.csv\")\n",
    "        profanity = set(pd.read_csv(file_path).values.ravel().tolist())\n",
    "        return profanity\n",
    "\n",
    "    #url regex\n",
    "    url_re = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\")\n",
    "    #email address regex\n",
    "    email_re = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
    "    #phone number regex\n",
    "    phone_re = re.compile(r'(?:(?:\\+?1\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?')\n",
    "    #ssn regex\n",
    "    ssn_re = re.compile(r'^(?!219-09-9999|078-05-1120)(?!666|000|9\\d{2})\\d{3}[-]?(?!00)\\d{2}[-]?(?!0{4})\\d{4}$')\n",
    "    #profanity regex\n",
    "    profanity_regex = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, get_profanity())))\n",
    "    \n",
    "    \n",
    "    doc = doc.lower()\n",
    "    \n",
    "    doc = profanity_regex.sub(\"criticaster\", doc)\n",
    "    doc = email_re.sub('blatherskite',doc)\n",
    "    doc = phone_re.sub('blatherskite',doc)\n",
    "    doc = ssn_re.sub('blatherskite',doc)\n",
    "    doc = url_re.sub('blatherskite',doc)\n",
    "    \n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "    # expand contractions    \n",
    "    if contraction_expansion:\n",
    "        doc = expand_contractions(doc)\n",
    "    # at least three characters long, cannot contain a number, and no more than 17 chars long\n",
    "    doc = re.findall(r'\\b[a-z][a-z][a-z]+\\b',doc)\n",
    "    doc = ' '.join(w for w in doc if w != 'nan' and len(w) <= 17)\n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        doc = lemmatize_text(doc)\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        doc = remove_stopwords(doc, is_lower_case=True)\n",
    "    if len(doc) == 0:\n",
    "        doc = \"spam\"    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewData():\n",
    "    best_model_path = os.path.join('best_estimators','label_best_estimator.pkl')\n",
    "    \n",
    "    \n",
    "    def __init__(self, recent_data_path, new_data_path, normalize_corpus):\n",
    "        self.recent_data_path = recent_data_path\n",
    "        self.new_data_path = new_data_path\n",
    "        self.normalize_corpus = normalize_corpus\n",
    "    \n",
    "    def get_data(self):\n",
    "        recent_data = pd.read_excel(self.recent_data_path)\n",
    "        last_date = pd.to_datetime(recent_data['Date']).max()\n",
    "        \n",
    "        new_data = pd.read_csv(new_data_path)\n",
    "        new_data = new_data.rename({'Please tell us what you value most about this website.':'Value Comment'},axis=1)\n",
    "        new_data['Date'] = pd.to_datetime(new_data['Date'])\n",
    "        new_data = new_data[new_data['Date'] > last_date].dropna(subset=['Value Comment'])\n",
    "        new_data['Normalized Value Comment'] = new_data['Value Comment'].apply(self.normalize_corpus)\n",
    "        \n",
    "        return new_data\n",
    "        \n",
    "    def predict(self):\n",
    "        with open(self.best_model_path, 'rb') as f: \n",
    "            pickled_model = pickle.load(f)  \n",
    "        \n",
    "        new_data = self.get_data()\n",
    "        preds = pickled_model.predict(new_data)\n",
    "        pred_probs = pickled_model.predict_proba(new_data)\n",
    "        \n",
    "\n",
    "        new_data['Predictions'] = preds\n",
    "        new_data['Ham Prediction Probability'] = 0\n",
    "        new_data['Spam Prediction Probability'] = 0\n",
    "        new_data[['Ham Prediction Probability', 'Spam Prediction Probability']] = pd.DataFrame(pred_probs).values\n",
    "        new_data['Prediction Probabilities Delta'] = abs(new_data['Ham Prediction Probability'] - new_data['Spam Prediction Probability'])\n",
    "        \n",
    "        classifier_results = new_data.sort_values(by='Prediction Probabilities Delta')[['Value Comment','Predictions','Prediction Probabilities Delta']]\n",
    "        \n",
    "        results_path = os.path.join('results','ClassificationResults.xlsx')\n",
    "        writer = pd.ExcelWriter(results_path)\n",
    "        classifier_results.to_excel(writer,'Classification Results')\n",
    "        writer.save()\n",
    "        return classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_data_path = os.path.join('labeled_data','labeled_values.xlsx')\n",
    "#insert name of newest data file below\n",
    "new_data_path = os.path.join('unlabeled_data','201806121828-USA.gov_Cus-1.1.csv')\n",
    "nd = NewData(recent_data_path, new_data_path, normalize_corpus)\n",
    "nd.best_model_path = os.path.join('best_estimators','label_best_estimator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = nd.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results = nd.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
